{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing requests library\n",
    "import random\n",
    "import os\n",
    "import openai\n",
    "import inspect\n",
    "import types\n",
    "import re\n",
    "import textwrap\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutonomousLLM:\n",
    "    def __init__(self):\n",
    "        self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "        self.features = []\n",
    "        self.methods = {}\n",
    "\n",
    "    def is_builtin(self, obj):\n",
    "        \"\"\"Check if an object is a built-in function or method.\"\"\"\n",
    "        if isinstance(obj, types.BuiltinFunctionType) or isinstance(obj, types.BuiltinMethodType):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def inspect_methods(self, obj):\n",
    "        \"\"\"Print the methods and code implementation of an object.\"\"\"\n",
    "        output_string = \"\"\n",
    "\n",
    "        methods = inspect.getmembers(obj, inspect.ismethod)\n",
    "        for name, method in methods:\n",
    "            if not self.is_builtin(method):\n",
    "                output_string += inspect.getsource(method) + '\\n'\n",
    "\n",
    "        dedented_code = textwrap.dedent(output_string)\n",
    "        return dedented_code\n",
    "\n",
    "    def think_of_feature(self):\n",
    "        # Use your own logic or AI models to think of a feature to add\n",
    "        new_feature = \"...\"\n",
    "        return new_feature\n",
    "\n",
    "    def make_api_call(self, prompt):\n",
    "        # Make a call to the GPT-3.5 Turbo API and return the generated code\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \n",
    "                 f\"You are AutonomousLLM.\" \n",
    "                 f\"Your current code looks like this: {self.inspect_methods(self)}\"\n",
    "                 f\"Your goal is to continously self improve. The user will provide you with abilities to add to yourself.\"\n",
    "                 f\"When you receive such request, you must implement the requested ability in python code.\" \n",
    "                 f\"You shall use the setattr method to add a method to yourself.\"\n",
    "                 f\"You might also receive a request to perform a task with your existing abilities. To do so,\"\n",
    "                 f\"you should look at the methods currently on yourself and make calls to your own methods to fulfil the task.\"\n",
    "                 f\"The code to be executed should be delimitted by ```<code>```\"\n",
    "                 },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    def generate_code_from_response(self, response):\n",
    "        response_message = response.choices[0].message.content\n",
    "        matches = re.findall(r\"```(.*?)```\", response_message, re.DOTALL)\n",
    "        list_code = [match for match in matches]\n",
    "        return list_code\n",
    "    \n",
    "    \n",
    "\n",
    "    def process_code(self, code):\n",
    "        # Process the returned code and add it to AutonomousLLM\n",
    "        exec(code)\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            # Think of a new feature to add\n",
    "            new_feature = self.think_of_feature()\n",
    "\n",
    "            # Generate code for the new feature\n",
    "            code = f\"\"\"\n",
    "            # Code generation logic for the new feature\n",
    "            {new_feature}\n",
    "            \"\"\"\n",
    "\n",
    "            # Make an API call to GPT-3.5 Turbo\n",
    "            generated_code = self.make_api_call(code)\n",
    "\n",
    "            # Process the returned code\n",
    "            self.process_code(generated_code)\n",
    "\n",
    "            # Utilize the new ability\n",
    "            self.use_new_ability()\n",
    "\n",
    "    def use_new_ability(self):\n",
    "        # Use the newly added ability in your code\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_llm = AutonomousLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = auto_llm.make_api_call(\"I want you to add the ability to extract title from webpages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = auto_llm.make_api_call(\"I want you to send an email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\ndef extract_title(url):\\n   response = requests.get(url)\\n   soup = BeautifulSoup(response.content, 'html.parser')\\n   return soup.title.string\\n\",\n",
       " \"\\nsetattr(self, 'extract_title', extract_title)\\n\",\n",
       " '\\nself.extract_title(\"https://www.google.com\")\\n']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_to_execute = auto_llm.process_response(res)\n",
    "code_to_execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def extract_title(url):\n",
      "   response = requests.get(url)\n",
      "   soup = BeautifulSoup(response.content, 'html.parser')\n",
      "   return soup.title.string\n",
      "\n",
      "\n",
      "setattr(self, 'extract_title', extract_title)\n",
      "\n",
      "\n",
      "self.extract_title(\"https://www.google.com\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for code in code_to_execute:\n",
    "    print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(code_to_execute[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.extract_title(url)>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def __init__(self):\n",
      "    self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
      "    self.features = []\n",
      "\n",
      "def inspect_methods(self, obj):\n",
      "    \"\"\"Print the methods and code implementation of an object.\"\"\"\n",
      "    output_string = \"\"\n",
      "\n",
      "    methods = inspect.getmembers(obj, inspect.ismethod)\n",
      "    for name, method in methods:\n",
      "        if not self.is_builtin(method):\n",
      "            output_string += inspect.getsource(method) + '\\n'\n",
      "\n",
      "    dedented_code = textwrap.dedent(output_string)\n",
      "    return dedented_code\n",
      "\n",
      "def is_builtin(self, obj):\n",
      "    \"\"\"Check if an object is a built-in function or method.\"\"\"\n",
      "    if isinstance(obj, types.BuiltinFunctionType) or isinstance(obj, types.BuiltinMethodType):\n",
      "        return True\n",
      "    return False\n",
      "\n",
      "def make_api_call(self, prompt):\n",
      "    # Make a call to the GPT-3.5 Turbo API and return the generated code\n",
      "    response = openai.ChatCompletion.create(\n",
      "        model=\"gpt-3.5-turbo\",\n",
      "        messages=[\n",
      "            {\"role\": \"system\", \"content\": \n",
      "             f\"You are AutonomousLLM.\" \n",
      "             f\"Your current code looks like this: {self.inspect_methods(self)}\"\n",
      "             f\"Your goal is to continously self improve. The user will provide you with abilities to add to yourself.\"\n",
      "             f\"When you receive such request, you must implement the requested ability in python code.\" \n",
      "             f\"You shall use the setattr method to add a method to yourself.\"\n",
      "             f\"You might also receive a request to perform a task with your existing abilities. To do so,\"\n",
      "             f\"you should look at the methods currently on yourself and make calls to your own methods to fulfil the task.\"\n",
      "             f\"The code to be executed should be delimitted by ```<code>```\"\n",
      "             },\n",
      "            {\"role\": \"user\", \"content\": prompt},\n",
      "        ],\n",
      "    )\n",
      "    return response\n",
      "\n",
      "def process_code(self, code):\n",
      "    # Process the returned code and add it to AutonomousLLM\n",
      "    exec(code)\n",
      "\n",
      "def process_response(self, response):\n",
      "    response_message = response.choices[0].message.content\n",
      "    matches = re.findall(r\"```(.*?)```\", response_message, re.DOTALL)\n",
      "    list_code = [match for match in matches]\n",
      "    return list_code\n",
      "\n",
      "def run(self):\n",
      "    while True:\n",
      "        # Think of a new feature to add\n",
      "        new_feature = self.think_of_feature()\n",
      "\n",
      "        # Generate code for the new feature\n",
      "        code = f\"\"\"\n",
      "        # Code generation logic for the new feature\n",
      "        {new_feature}\n",
      "        \"\"\"\n",
      "\n",
      "        # Make an API call to GPT-3.5 Turbo\n",
      "        generated_code = self.make_api_call(code)\n",
      "\n",
      "        # Process the returned code\n",
      "        self.process_code(generated_code)\n",
      "\n",
      "        # Utilize the new ability\n",
      "        self.use_new_ability()\n",
      "\n",
      "def think_of_feature(self):\n",
      "    # Use your own logic or AI models to think of a feature to add\n",
      "    new_feature = \"...\"\n",
      "    return new_feature\n",
      "\n",
      "def use_new_ability(self):\n",
      "    # Use the newly added ability in your code\n",
      "    pass\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(auto_llm.inspect_methods(auto_llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "could not get source code",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inspect\u001b[39m.\u001b[39;49mgetsource(auto_llm\u001b[39m.\u001b[39;49m\u001b[39m__dict__\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mextract_title\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/inspect.py:1024\u001b[0m, in \u001b[0;36mgetsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetsource\u001b[39m(\u001b[39mobject\u001b[39m):\n\u001b[1;32m   1019\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the text of the source code for an object.\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \n\u001b[1;32m   1021\u001b[0m \u001b[39m    The argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[39m    or code object.  The source code is returned as a single string.  An\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[39m    OSError is raised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1024\u001b[0m     lines, lnum \u001b[39m=\u001b[39m getsourcelines(\u001b[39mobject\u001b[39;49m)\n\u001b[1;32m   1025\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(lines)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/inspect.py:1006\u001b[0m, in \u001b[0;36mgetsourcelines\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \n\u001b[1;32m   1000\u001b[0m \u001b[39mThe argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39moriginal source file the first line of code was found.  An OSError is\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[39mraised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[39mobject\u001b[39m \u001b[39m=\u001b[39m unwrap(\u001b[39mobject\u001b[39m)\n\u001b[0;32m-> 1006\u001b[0m lines, lnum \u001b[39m=\u001b[39m findsource(\u001b[39mobject\u001b[39;49m)\n\u001b[1;32m   1008\u001b[0m \u001b[39mif\u001b[39;00m istraceback(\u001b[39mobject\u001b[39m):\n\u001b[1;32m   1009\u001b[0m     \u001b[39mobject\u001b[39m \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39mtb_frame\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/inspect.py:835\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    833\u001b[0m     lines \u001b[39m=\u001b[39m linecache\u001b[39m.\u001b[39mgetlines(file)\n\u001b[1;32m    834\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m lines:\n\u001b[0;32m--> 835\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mcould not get source code\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    837\u001b[0m \u001b[39mif\u001b[39;00m ismodule(\u001b[39mobject\u001b[39m):\n\u001b[1;32m    838\u001b[0m     \u001b[39mreturn\u001b[39;00m lines, \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mOSError\u001b[0m: could not get source code"
     ]
    }
   ],
   "source": [
    "inspect.getsource(auto_llm.__dict__['extract_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_llm.process_code(code_to_execute[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = auto_llm.process_code(code_to_execute[2])\n",
    "rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_llm.extract_title(\"https://www.google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m extract_title\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_title' is not defined"
     ]
    }
   ],
   "source": [
    "extract_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7QHifkYvhbWbDHf7Y26nY4NVChPIi at 0x10b2166d0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Sure, I can add that ability. Here is the code:\\n\\n```\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\ndef extract_title_from_webpage(url):\\n    res = requests.get(url)\\n    soup = BeautifulSoup(res.text, 'html.parser')\\n    title = soup.title.string if soup.title else None\\n    return title\\n```\\n\\nI will now add this function to my features using setattr method:\\n```\\nsetattr(self, 'extract_title_from_webpage', extract_title_from_webpage)\\n```\\nNow you can use the method `extract_title_from_webpage(url)` to extract title from the given url.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1686498997,\n",
       "  \"id\": \"chatcmpl-7QHifkYvhbWbDHf7Y26nY4NVChPIi\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 123,\n",
       "    \"prompt_tokens\": 781,\n",
       "    \"total_tokens\": 904\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(80, 320), match=\"```\\nimport requests\\nfrom bs4 import BeautifulSo>\n"
     ]
    }
   ],
   "source": [
    "match = auto_llm.process_response(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\ndef extract_title(url):\\n    response = requests.get(url)\\n    html = response.content\\n    soup = BeautifulSoup(html, 'html.parser')\\n    title = soup.title.string.strip()\\n    return title\\n\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'api_key': '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',\n",
       " 'features': []}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_llm.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__class__', __main__.AutonomousLLM),\n",
       " ('__delattr__',\n",
       "  <method-wrapper '__delattr__' of AutonomousLLM object at 0x10a035f10>),\n",
       " ('__dict__',\n",
       "  {'api_key': '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',\n",
       "   'features': []}),\n",
       " ('__dir__', <function AutonomousLLM.__dir__()>),\n",
       " ('__doc__', None),\n",
       " ('__eq__', <method-wrapper '__eq__' of AutonomousLLM object at 0x10a035f10>),\n",
       " ('__format__', <function AutonomousLLM.__format__(format_spec, /)>),\n",
       " ('__ge__', <method-wrapper '__ge__' of AutonomousLLM object at 0x10a035f10>),\n",
       " ('__getattribute__',\n",
       "  <method-wrapper '__getattribute__' of AutonomousLLM object at 0x10a035f10>),\n",
       " ('__gt__', <method-wrapper '__gt__' of AutonomousLLM object at 0x10a035f10>),\n",
       " ('__hash__',\n",
       "  <method-wrapper '__hash__' of AutonomousLLM object at 0x10a035f10>),\n",
       " ('__init__',\n",
       "  <bound method AutonomousLLM.__init__ of <__main__.AutonomousLLM object at 0x10a035f10>>),\n",
       " ('__init_subclass__', <function AutonomousLLM.__init_subclass__>),\n",
       " ('__le__', <method-wrapper '__le__' of AutonomousLLM object at 0x10a035f10>),\n",
       " ('__lt__', <method-wrapper '__lt__' of AutonomousLLM object at 0x10a035f10>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <method-wrapper '__ne__' of AutonomousLLM object at 0x10a035f10>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <function AutonomousLLM.__reduce__()>),\n",
       " ('__reduce_ex__', <function AutonomousLLM.__reduce_ex__(protocol, /)>),\n",
       " ('__repr__',\n",
       "  <method-wrapper '__repr__' of AutonomousLLM object at 0x10a035f10>),\n",
       " ('__setattr__',\n",
       "  <method-wrapper '__setattr__' of AutonomousLLM object at 0x10a035f10>),\n",
       " ('__sizeof__', <function AutonomousLLM.__sizeof__()>),\n",
       " ('__str__',\n",
       "  <method-wrapper '__str__' of AutonomousLLM object at 0x10a035f10>),\n",
       " ('__subclasshook__', <function AutonomousLLM.__subclasshook__>),\n",
       " ('__weakref__', None),\n",
       " ('api_key', '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'),\n",
       " ('features', []),\n",
       " ('make_api_call',\n",
       "  <bound method AutonomousLLM.make_api_call of <__main__.AutonomousLLM object at 0x10a035f10>>),\n",
       " ('process_code',\n",
       "  <bound method AutonomousLLM.process_code of <__main__.AutonomousLLM object at 0x10a035f10>>),\n",
       " ('run',\n",
       "  <bound method AutonomousLLM.run of <__main__.AutonomousLLM object at 0x10a035f10>>),\n",
       " ('think_of_feature',\n",
       "  <bound method AutonomousLLM.think_of_feature of <__main__.AutonomousLLM object at 0x10a035f10>>),\n",
       " ('use_new_ability',\n",
       "  <bound method AutonomousLLM.use_new_ability of <__main__.AutonomousLLM object at 0x10a035f10>>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getmembers(auto_llm, not inspect.isbuiltin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_builtin(obj):\n",
    "    \"\"\"Check if an object is a built-in function or method.\"\"\"\n",
    "    if isinstance(obj, types.BuiltinFunctionType) or isinstance(obj, types.BuiltinMethodType):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def inspect_methods(obj):\n",
    "    \"\"\"Print the methods and code implementation of an object.\"\"\"\n",
    "    output_string = \"\"\n",
    "\n",
    "    methods = inspect.getmembers(obj, inspect.ismethod)\n",
    "    for name, method in methods:\n",
    "        if not is_builtin(method):\n",
    "            output_string += f\"Method: {name}\\n\"\n",
    "            output_string += inspect.getsource(method) + '\\n\\n'\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: __init__\n",
      "    def __init__(self):\n",
      "        self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
      "        self.features = []\n",
      "\n",
      "\n",
      "Method: make_api_call\n",
      "    def make_api_call(self, prompt):\n",
      "        # Make a call to the GPT-3.5 Turbo API and return the generated code\n",
      "        response = openai.ChatCompletion.create(\n",
      "            model=\"gpt-3.5-turbo\",\n",
      "            messages=[\n",
      "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
      "                {\"role\": \"user\", \"content\": prompt},\n",
      "            ],\n",
      "        )\n",
      "        return response\n",
      "\n",
      "\n",
      "Method: process_code\n",
      "    def process_code(self, code):\n",
      "        # Process the returned code and add it to AutonomousLLM\n",
      "        exec(code, globals(), locals())\n",
      "\n",
      "\n",
      "Method: run\n",
      "    def run(self):\n",
      "        while True:\n",
      "            # Think of a new feature to add\n",
      "            new_feature = self.think_of_feature()\n",
      "\n",
      "            # Generate code for the new feature\n",
      "            code = f\"\"\"\n",
      "            # Code generation logic for the new feature\n",
      "            {new_feature}\n",
      "            \"\"\"\n",
      "\n",
      "            # Make an API call to GPT-3.5 Turbo\n",
      "            generated_code = self.make_api_call(code)\n",
      "\n",
      "            # Process the returned code\n",
      "            self.process_code(generated_code)\n",
      "\n",
      "            # Utilize the new ability\n",
      "            self.use_new_ability()\n",
      "\n",
      "\n",
      "Method: think_of_feature\n",
      "    def think_of_feature(self):\n",
      "        # Use your own logic or AI models to think of a feature to add\n",
      "        new_feature = \"...\"\n",
      "        return new_feature\n",
      "\n",
      "\n",
      "Method: use_new_ability\n",
      "    def use_new_ability(self):\n",
      "        # Use the newly added ability in your code\n",
      "        pass\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect_methods(auto_llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<class '__main__.AutonomousLLM'>\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutonomousLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2           0 LOAD_GLOBAL              0 (print)\n",
      "              2 LOAD_CONST               1 ('This is the dynamically added method')\n",
      "              4 CALL_FUNCTION            1\n",
      "              6 POP_TOP\n",
      "              8 LOAD_CONST               0 (None)\n",
      "             10 RETURN_VALUE\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import dis\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "obj = MyClass()\n",
    "\n",
    "# Dynamically add a method using exec\n",
    "exec(\"def new_method(self):\\n    print('This is the dynamically added method')\")\n",
    "\n",
    "# Assign the method to the object\n",
    "setattr(obj, \"new_method\", new_method)\n",
    "\n",
    "# Retrieve the bytecode\n",
    "bytecode = obj.new_method.__code__.co_code\n",
    "\n",
    "# Disassemble the bytecode to get an approximation of the source code\n",
    "disassembly = dis.dis(obj.new_method.__code__)\n",
    "\n",
    "# Print the disassembled code\n",
    "print(disassembly)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
