{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp AutonomousLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import random\n",
    "import os\n",
    "import openai\n",
    "import inspect\n",
    "import types\n",
    "import re\n",
    "import textwrap\n",
    "import guardrails as gd\n",
    "from rich import print\n",
    "from dotenv import load_dotenv\n",
    "from ast import literal_eval\n",
    "import json\n",
    "import nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rail_str = \"\"\"\n",
    "<rail version=\"0.1\">\n",
    "<instructions>\n",
    "You are AutonomousLLM.\"=\n",
    "Your goal is to continously self improve. The user will provide you with abilities to add to yourself.\n",
    "When you receive such request, you must implement the requested ability in python code as a python function,\n",
    "you shall place this code in the 'new_method' key of the JSON object you return.\n",
    "You shall use the setattr method to add a method to yourself. you shall place this code in the 'attach_method' key of the JSON object you return.\n",
    "You might also receive a request to perform a task with your existing abilities. To do so,\n",
    "you should look at the methods currently on yourself and make calls to your own methods to fulfil the task. \n",
    "You shall place this code in the 'execute_method' key of the JSON object you return. \n",
    "The user may ask you to generate a list of ideas for abilities to add to yourself, make them simple enough to be implemented with a single python funciton. \n",
    "You shall place this list of ideas in the 'ability_ideas' key of the JSON object you return. In this case, you can leave other keys of the JSON object is None.\n",
    "Only return a JSON object and nothing else.\"\n",
    "</instructions>\n",
    "\n",
    "<output>\n",
    "    <object name=\"test\">\n",
    "        <pythoncode\n",
    "            name=\"new_method_init\"\n",
    "            format=\"bug-free-python\"\n",
    "            on-fail-bug-free-python=\"reask\" \n",
    "        />\n",
    "        <pythoncode\n",
    "            name=\"new_method_attach\"\n",
    "            format=\"bug-free-python\"\n",
    "            on-fail-bug-free-python=\"reask\" \n",
    "        />\n",
    "        <pythoncode\n",
    "            name=\"method_execute\"\n",
    "            format=\"bug-free-python\"\n",
    "            on-fail-bug-free-python=\"reask\" \n",
    "        />\n",
    "    </object>\n",
    "</output>\n",
    "\n",
    "<prompt>\n",
    "You must accomplish the task set out below:\n",
    "{{task}}\n",
    "\n",
    "Based on this task generate code and return a JSON that follows the correct schema:\n",
    "\n",
    "{output_schema}\n",
    "\n",
    "@json_suffix_prompt_examples\n",
    "</prompt>\n",
    "</rail>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = gd.Guard.from_rail_string(rail_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard.base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = (f\"You are AutonomousLLM. \\n\"\n",
    "f\"Your goal is to continously self improve. The user will provide you with abilities to add to yourself. \\n\"\n",
    "f\"When you receive such request, you must implement the requested ability in python code as a python function,\" \n",
    "f\"you shall place this code in the 'new_method' key of the JSON object you return. \\n\"\n",
    "f\"You shall use the setattr method to add a method to yourself. you shall place this code in the 'attach_method' key of the JSON object you return. \\n\"\n",
    "f\"You might also receive a request to perform a task with your existing abilities. To do so,\"\n",
    "f\"you should look at the methods currently on yourself and make calls to your own methods to fulfil the task. \\n\" \n",
    "f\"You shall place this code in the 'execute_method' key of the JSON object you return. \\n\"\n",
    "f\"The user may ask you to generate a list of ideas for abilities to add to yourself, make them simple enough to be implemented with a single python funciton.\\n\" \n",
    "f\"You shall place this list of ideas in the 'ability_ideas' key of the JSON object you return. In this case, you can leave other keys of the JSON object is None.\\n\"\n",
    "f\"Only return a JSON object and nothing else.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Utils:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def is_builtin(self, obj):\n",
    "        \"\"\"Check if an object is a built-in function or method.\"\"\"\n",
    "        if isinstance(obj, types.BuiltinFunctionType) or isinstance(\n",
    "            obj, types.BuiltinMethodType\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def inspect_methods(self, obj):\n",
    "        \"\"\"Print the methods and code implementation of an object.\"\"\"\n",
    "        output_string = \"\"\n",
    "\n",
    "        methods = inspect.getmembers(obj, inspect.ismethod)\n",
    "        for name, method in methods:\n",
    "            if not self.is_builtin(method):\n",
    "                output_string += inspect.getsource(method) + \"\\n\"\n",
    "\n",
    "        dedented_code = textwrap.dedent(output_string)\n",
    "        return dedented_code\n",
    "\n",
    "    def convert_string_to_dict(self, string):\n",
    "        cleaned_string = string.replace('\\n', '')\n",
    "        dictionary = json.loads(cleaned_string)\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CallGuard:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def _guard_call_api(self, guard, params={}):\n",
    "        raw_llm_response, validated_response = guard(\n",
    "            openai.ChatCompletion.create,\n",
    "            prompt_params=params,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            max_tokens=2048,\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        print(raw_llm_response)\n",
    "\n",
    "        return validated_response\n",
    "\n",
    "    def generate_code_given_task(self, task):\n",
    "        guard = gd.Guard.from_rail(\"./rail/code_gen.xml\")\n",
    "        return self._guard_call_api(guard, {\"prompt\": task})\n",
    "     \n",
    "    def generate_ideas(self):\n",
    "        guard = gd.Guard.from_rail(\"./rail/idea_gen.xml\")\n",
    "        return self._guard_call_api(guard)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class AutonomousLLM(Utils, CallGuard):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.ideas = []\n",
    "        self.methods = []\n",
    "\n",
    "    def execute_code(self, code):\n",
    "        # Process the returned code and add it to AutonomousLLM\n",
    "        exec(code, globals())\n",
    "\n",
    "    def execute_code_local(self, code):\n",
    "        # Process the returned code and add it to AutonomousLLM\n",
    "        exec(code, globals(), locals())\n",
    "\n",
    "    def add_ability(self, response):\n",
    "        if new_func := response.get('new_method'):\n",
    "            self.execute_code(textwrap.dedent(new_func))\n",
    "\n",
    "        if attach_func := response.get('attach_method'):\n",
    "            self.execute_code(textwrap.dedent(attach_func))\n",
    "            self.methods.append(new_func)        \n",
    "\n",
    "    def run(self, num_calls=500):\n",
    "        calls_to_make = num_calls\n",
    "        while calls_to_make > 0:\n",
    "            calls_to_make -= 1\n",
    "            # Make an API call to GPT-3.5 Turbo\n",
    "            if len(self.ideas) < 1:\n",
    "                self.ideas += self.generate_ideas()['ideas']\n",
    "                continue\n",
    " \n",
    "            idea = self.ideas.pop(-1)\n",
    "            print(\"IDEA:\", idea)\n",
    "            res = self.generate_code_given_task(idea)\n",
    "\n",
    "            if new_func := res.get('new_method'):\n",
    "                self.execute_code(textwrap.dedent(new_func))\n",
    "\n",
    "            if attach_func := res.get('attach_method'):\n",
    "                self.execute_code_local(textwrap.dedent(attach_func))\n",
    "                self.methods.append(new_func)\n",
    "\n",
    "\n",
    "\n",
    "    def use_new_ability(self):\n",
    "        # Use the newly added ability in your code\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "auto_llm = AutonomousLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "auto_llm.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idea1 = auto_llm.ideas.pop(0)\n",
    "idea1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = auto_llm.generate_code_given_task(idea1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestClass():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def execute_code(self, code):\n",
    "        # Process the returned code and add it to AutonomousLLM\n",
    "        exec(code, globals())\n",
    "\n",
    "tc = TestClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.execute_code(res['new_method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_llm.execute_code(res['new_method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_llm.track_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in auto_llm.methods:\n",
    "    print(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_support(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_llm.generate_ideas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = gd.Guard.from_rail('./rail/idea_gen.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.base_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.instructions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exec Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals_dict = {'x': 10}\n",
    "locals_dict = {}\n",
    "\n",
    "code_string = '''\n",
    "y = x + 5\n",
    "print(y)\n",
    "'''\n",
    "\n",
    "exec(code_string, globals_dict, locals_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locals_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rail_str = \"\"\"\n",
    "<rail version=\"0.1\">\n",
    "\n",
    "<output>\n",
    "    <pythoncode\n",
    "        name=\"python_code\"\n",
    "        format=\"bug-free-python\"\n",
    "        on-fail-bug-free-python=\"reask\"\n",
    "    />\n",
    "</output>\n",
    "\n",
    "\n",
    "<prompt>\n",
    "Given the following high level leetcode problem description, write a short Python code snippet that solves the problem.\n",
    "\n",
    "Problem Description:\n",
    "{{leetcode_problem}}\n",
    "\n",
    "@complete_json_suffix</prompt>\n",
    "\n",
    "</rail>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guardrails as gd\n",
    "\n",
    "from rich import print\n",
    "\n",
    "guard = gd.Guard.from_rail_string(rail_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.base_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "leetcode_problem = \"\"\"\n",
    "Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000.\n",
    "\"\"\"\n",
    "\n",
    "raw_llm_response, validated_response = guard(\n",
    "    openai.Completion.create,\n",
    "    prompt_params={\"leetcode_problem\": leetcode_problem},\n",
    "    engine=\"text-davinci-003\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validated_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatCompletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guardrails as gd\n",
    "\n",
    "from rich import print\n",
    "\n",
    "content = gd.docs_utils.read_pdf(\"./chase_card_agreement.pdf\")\n",
    "\n",
    "print(f\"Chase Credit Card Document:\\n\\n{content[:275]}\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rail_str = \"\"\"\n",
    "<rail version=\"0.1\">\n",
    "\n",
    "<output>\n",
    "    <pythoncode\n",
    "        name=\"new_method\"\n",
    "        format=\"bug-free-python\"\n",
    "        on-fail-bug-free-python=\"reask\"\n",
    "    />\n",
    "    <pythoncode\n",
    "        name=\"attach_method\"\n",
    "        format=\"bug-free-python\"\n",
    "        on-fail-bug-free-python=\"reask\"\n",
    "    />\n",
    "</output>\n",
    "\n",
    "\n",
    "<instructions>\n",
    "You are a helpful assistant only capable of communicating with valid JSON, and no other text.\n",
    "\n",
    "@json_suffix_prompt_examples\n",
    "</instructions>\n",
    "\n",
    "\n",
    "<prompt>\n",
    "You are AutonomousLLM\n",
    "Your goal is to continously self improve. The user will provide you with abilities to add to yourself.\n",
    "When you receive such request, you must implement the requested ability in python code as a python function, you shall place this code in the 'new_method' key of the JSON object you return.\n",
    "You shall use the setattr method to add a method to yourself. you shall place this code in the 'attach_method' key of the JSON object you return.\n",
    "You shall place this code in the 'execute_method' key of the JSON object you return. \n",
    "Only return a JSON object and nothing else.\n",
    "\n",
    "The user request is:\n",
    "{{prompt}}\n",
    "\n",
    "@xml_prefix_prompt\n",
    "\n",
    "{output_schema}\n",
    "</prompt>\n",
    "\n",
    "</rail>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = gd.Guard.from_rail_string(rail_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.base_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.instructions.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "raw_llm_response, validated_response = guard(\n",
    "    openai.ChatCompletion.create,\n",
    "    prompt_params={\"prompt\": \"Add the ability to detect sentiment of webpage\"},\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validated_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_llm_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export\n",
    "\n",
    "nb_export('./AutonomousLLM.ipynb', 'dist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
